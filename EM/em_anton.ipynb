{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WMlNHfVxBEDT"
   },
   "source": [
    "# Expectation-maximization algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "icS4MsxIBEDU"
   },
   "source": [
    "We will derive and implement formulas for Gaussian Mixture Model â€” one of the most commonly used methods for performing soft clustering of the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Place samples.npz file in the same folder as em_anton.ipynb "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible display problems\n",
    "\n",
    "If some of the formulas are not displayed correctly, you can view the file [here](https://nbviewer.jupyter.org/github/antonsavostianov/Bayes-in-ML/blob/master/EM/em_anton.ipynb) via nbviewer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2940,
     "status": "ok",
     "timestamp": 1594367897295,
     "user": {
      "displayName": "Anton Savostianov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjBbdK6s_Mn01lJaBnVOEN8yDfMrLpHYHBGDcZ8Uw=s64",
      "userId": "02854421325018836691"
     },
     "user_tz": -120
    },
    "id": "urylZcbeBEDc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import slogdet, det, solve\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy.special import softmax\n",
    "\n",
    "from numpy.random import default_rng\n",
    "# from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dL3A2sntBEDj"
   },
   "source": [
    "## Implementing EM for GMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0xPS_VdpBEDk"
   },
   "source": [
    "For debugging, we will use samples from a Gaussian mixture model with unknown mean, variance, and priors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1090,
     "status": "ok",
     "timestamp": 1594367912398,
     "user": {
      "displayName": "Anton Savostianov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjBbdK6s_Mn01lJaBnVOEN8yDfMrLpHYHBGDcZ8Uw=s64",
      "userId": "02854421325018836691"
     },
     "user_tz": -120
    },
    "id": "g9_aOn94BEDl",
    "outputId": "419b7854-1f94-478e-d9cf-3e81e5be7b02"
   },
   "outputs": [],
   "source": [
    "samples = np.load('samples.npz')\n",
    "X = samples['data']\n",
    "pi0 = samples['pi0']\n",
    "mu0 = samples['mu0']\n",
    "sigma0 = samples['sigma0']\n",
    "plt.scatter(X[:, 0], X[:, 1], c='grey', s=30)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jWjmxuKna_5b"
   },
   "outputs": [],
   "source": [
    "type(samples)\n",
    "print(samples.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 846,
     "status": "ok",
     "timestamp": 1594367631313,
     "user": {
      "displayName": "Anton Savostianov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjBbdK6s_Mn01lJaBnVOEN8yDfMrLpHYHBGDcZ8Uw=s64",
      "userId": "02854421325018836691"
     },
     "user_tz": -120
    },
    "id": "RuJo5_9Ga_5e",
    "outputId": "edacf818-590b-41f8-9b2a-9a589dfaa570"
   },
   "outputs": [],
   "source": [
    "print(\"X.shape\",X.shape)\n",
    "print(\"X[0]\",X[0])\n",
    "print(\"sigma0.shape\",sigma0.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical Model\n",
    "\n",
    "We aim to seek for the probability density of the data as a mixture of simpler distributions (in this particular case - Gaussians). Each Gaussian component determines its cluster (class). The number of Gaussians (= the number of clusters) to use is our choice (in our example it is $3$). Assuming that samples are independent the density reads\n",
    "\n",
    "$$\n",
    "p(X,T|\\theta)=\\prod_{i=1}^n p(x_i,t_i|\\theta)=\\prod_{i=1}^n p(x_i|t_i,\\theta)p(t_i|\\theta)=\\prod_{i=1}^n f_{\\!\\mathcal{N}}(x_i| \\mu_{t_i}, \\Sigma_{t_i})\\pi_{t_i},\n",
    "$$\n",
    "\n",
    "where $X=\\{x_i\\}_{i=1}^n$ - sample points ($x_i\\in{\\mathbb R}^d$), $T=\\{t_i\\}^n_{i=1}$ - the class of the corresponding point ($t_i\\in\\{1,2,...,C\\}$, $t$ for tag),\n",
    "\n",
    "$f_{\\!\\mathcal{N}}(x| \\mu, \\Sigma)= \\frac{1}{(2\\pi)^{d/2}\\sqrt{|det \\Sigma|}}exp\\Big(-\\frac{1}{2}(x-\\mu)^t\\Sigma^{-1}(x-\\mu)\\Big)$ - density of a normal distribution with mean $\\mu$ and covariance matrix $\\Sigma$,\n",
    "\n",
    "$\\theta = \\{\\pi_{c},\\mu_{c},\\Sigma_{c}\\}_{c=1}^C$ - parameters of our model: $\\pi_{c}$ - probability of belonging to the class $c$, $(\\mu_{c},\\Sigma_{c})$ - mean and covariance of the Gaussian corresponding to the class $c$.\n",
    "\n",
    "The fact that $p(t_i|\\theta)$ does not depend on $\\theta$ ($p(t_i|\\theta)=\\pi_{t_i}$) is our model assumption.\n",
    "\n",
    "Notice that if we knew the tag $t_i$ for each point we could separately consider the points of each class $c$, and consequently estimate the parameters\n",
    "$\\mu_c,\\ \\Sigma_c$ (via sample mean and covariance matrix) and find $\\pi_c = \\frac{\\#\\{t_i=c\\}}{n}$. The main difficulty is that we do not know the tags! \n",
    "\n",
    "\n",
    "## Setting optimization problem\n",
    "So we consider $p(X|\\theta)$ and according to the maxlikelihood approach we should maximize it with respect to $\\theta$\n",
    "$$\n",
    "\\log\\, p(X|\\theta)\\to \\max\\limits_{\\theta}.\n",
    "$$\n",
    "We first rewrite $\\log p(X|\\theta)$ using an auxiliary distribution $q(t_i)$ over $\\{1,...,C\\}$:\n",
    "\n",
    "$$\n",
    "q(t_i):\\{1,...,C\\}\\to[0,1],\\ q(t_i)(c):=q(t_i=c), \\sum_{c=1}^Cq(t_i=c)=1. \n",
    "$$\n",
    "\n",
    "We have\n",
    "\n",
    "\\begin{multline}\n",
    "\\log(p(X\\mid\\theta))=\\sum_{i=1}^N\\log p(x_i\\mid \\theta) = \\sum_{i=1}^N\\sum_{c=1}^C q(t_i=c)\\log p(x_i\\mid \\theta)= \\sum_{i=1}^N\\sum_{c=1}^C q(t_i=c)\\log \\left(\\frac{p(x_i\\mid \\theta) p(x_i, t_i = c\\mid \\theta)}{p(x_i, t_i = c\\mid \\theta)}\\right) = \\\\\n",
    "\\sum_{i=1}^N\\sum_{c=1}^C q(t_i=c)\\log \\left(\\frac{p(x_i, t_i = c\\mid \\theta)}{p(t_i = c\\mid x_i, \\theta)}\\right) = \\sum_{i=1}^N\\sum_{c=1}^C q(t_i=c)\\log \\left(\\frac{q(t_i=c)}{p(t_i = c\\mid x_i, \\theta)}\\right) + \\sum_{i=1}^N\\sum_{c=1}^C q(t_i=c)\\log \\left(\\frac{p(x_i, t_i = c\\mid \\theta)}{q(t_i=c)}\\right)=\\\\\n",
    "\\sum_{i=1}^N \\mathcal{KL}(q(t_i)\\mid\\mid p(t_i|x_i,\\theta)) + \\sum_{i=1}^N\\sum_{c=1}^C q(t_i=c)\\log \\left(\\frac{p(x_i, t_i = c\\mid \\theta)}{q(t_i=c)}\\right):=\\mathcal{KL}(q(T)|| p(T|X,\\theta))+\\mathcal L(\\theta,q).\n",
    "\\end{multline}\n",
    "\n",
    "That is \n",
    "\n",
    "$$\n",
    "\\log(p(X\\mid\\theta)) = \\mathcal{KL}(q(T)|| p(T|X,\\theta))+\\mathcal L(\\theta,q).\n",
    "$$\n",
    "\n",
    "Since $\\mathcal{KL}(q(T)|| p(T|X,\\theta))\\geq 0$ (can be checked) we see that $\\log(p(X\\mid\\theta))\\geq \\mathcal L(\\theta,q)$. It is known that $\\mathcal L(\\theta,q)$ is a variational lower bound for $\\log p(X|\\theta)$. Therefore instead of the original problem we consider the new one\n",
    "\n",
    "$$\n",
    "\\mathcal L(\\theta,q)\\to \\max\\limits_{\\theta,q}.\n",
    "$$\n",
    "\n",
    "To maximize $\\mathcal L(\\theta,q)$ we do coordinatewise updates:\n",
    "\n",
    "\\begin{equation*}\n",
    "q_k=argmax_q\\mathcal L(\\theta_{k-1},q),\\ \\theta_k=argmax_\\theta \\mathcal L(\\theta,q_k). \n",
    "\\end{equation*}\n",
    "\n",
    "Let us consider each step seaparately:\n",
    "\n",
    "<b>E-step</b>: due to $\\log(p(X\\mid\\theta)) = \\mathcal{KL}(q(T)|| p(T|X,\\theta))+\\mathcal L(\\theta,q)$ we have<br>\n",
    "$\\mathcal{L}(\\theta, q) \\to \\max\\limits_{q} \\Leftrightarrow  \\mathcal{KL} (q(T) \\,||\\, p(T|X, \\theta)) \\to \\min \\limits_{q} \\Rightarrow q(T) = p(T|X, \\theta)$<br>\n",
    "<b>M-step</b>:<br> \n",
    "$\\mathcal{L}(\\theta, q) \\to \\max\\limits_{\\theta} \\Leftrightarrow \\mathbb{E}_{q(T)}\\log p(X,T | \\theta) \\to \\max\\limits_{\\theta}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ELmi1nAtBEDr"
   },
   "source": [
    "### E-step\n",
    "In this step we need to estimate the posterior distribution over the latent variables with fixed values of parameters: $q(t_i) = p(t_i \\mid x_i, \\theta)$. We assume that $t_i$ equals to the cluster index of the true component of the $x_i$ object. To do so we need to compute $\\gamma_{ic} = p(t_i = c \\mid x_i, \\theta)$. Note that $\\sum\\limits_{c=1}^C\\gamma_{ic}=1$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I0QKgIjwa_5m"
   },
   "source": [
    "\\begin{align}\n",
    "\\gamma_{ic} = p(t_i = c \\mid x_i, \\theta) = \\frac{p(x_i,\\theta \\mid t_i= c)\\,p(t_i=c)}{p(x_i,\\theta)} = \\frac{p(x_i\\mid t_i= c, \\theta )\\,p(\\theta\\mid t_i=c)\\,p(t_i=c)}{p(x_i,\\theta)}= |\\theta\\ and\\ t_i - independent|=\\\\\n",
    "\\frac{p(x_i\\mid t_i= c, \\theta )\\,p(\\theta)\\,p(t_i=c)}{\\sum\\limits_{c'=1}^C p(x_i,t_i=c', \\theta)} = \\frac{p(x_i\\mid t_i= c, \\theta )\\,p(\\theta)\\,p(t_i=c)}{\\sum\\limits_{c'=1}^C p(x_i \\mid t_i=c', \\theta)p(t_i=c'\\mid \\theta)p(\\theta)} = \\frac{p(x_i\\mid t_i= c, \\theta )\\pi_c}{\\sum\\limits_{c'=1}^C p(x_i \\mid t_i=c', \\theta)\\pi_{c'}} \n",
    "\\end{align}\n",
    "\n",
    "Now we compute\n",
    "\\begin{align}\n",
    "p(x_i\\mid t_i= c, \\theta )\\pi_c = & f_{\\mathcal{N}}(x_i\\mid \\mu_c,\\Sigma_c)\\pi_c = \\frac{1}{((2\\pi)^d|\\det\\Sigma_c|)^{1/2}}\\exp\\left(-\\frac{1}{2}\\Sigma_c^{-1}(x_i-\\mu_c).(x_i-\\mu_c)\\right)\\pi_c = \\\\\n",
    "& \\exp\\left(-\\frac{1}{2}\\Sigma_c^{-1}(x_i-\\mu_c).(x_i-\\mu_c) + \\log(\\pi_c) - \\frac{d}{2}\\log(2\\pi) - \\frac{1}{2}\\log|\\det\\Sigma_c| \\right).\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "53FR1RJ9BEDs"
   },
   "source": [
    "<b>usefull functions: </b> <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.slogdet.html\">```slogdet```</a> and <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.det.html#numpy.linalg.det\">```det```</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 891,
     "status": "ok",
     "timestamp": 1594367924392,
     "user": {
      "displayName": "Anton Savostianov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjBbdK6s_Mn01lJaBnVOEN8yDfMrLpHYHBGDcZ8Uw=s64",
      "userId": "02854421325018836691"
     },
     "user_tz": -120
    },
    "id": "wCUCaD28BEDw"
   },
   "outputs": [],
   "source": [
    "def E_step(X, pi, mu, sigma):\n",
    "    \"\"\"\n",
    "    Performs E-step on GMM model\n",
    "    Each input is numpy array:\n",
    "    X: (N x d), data points\n",
    "    pi: (C), mixture component weights \n",
    "    mu: (C x d), mixture component means\n",
    "    sigma: (C x d x d), mixture component covariance matrices\n",
    "    \n",
    "    Returns:\n",
    "    gamma: (N x C), probabilities of clusters for objects\n",
    "    \"\"\"\n",
    "    N = X.shape[0] # number of objects\n",
    "    C = pi.shape[0] # number of clusters\n",
    "    d = mu.shape[1] # dimension of each object\n",
    "    gamma = np.zeros((N, C)) # distribution q(T)\n",
    "    \n",
    "    log_sigma = np.linalg.slogdet(sigma)[1]\n",
    "    \n",
    "    logits = np.zeros((C,N))\n",
    "    \n",
    "    for c in range(C):\n",
    "        yc = np.linalg.solve(sigma[c],(X - mu[c]).T).T\n",
    "        logits[c] = -0.5 * np.sum(yc*(X-mu[c]),axis = 1) + np.log(pi[c])-np.log(2*np.pi) - 0.5*log_sigma[c]\n",
    "    logits = logits.T\n",
    "\n",
    "    gamma = softmax(logits, axis = 1)\n",
    "    \n",
    "    return gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 936,
     "status": "ok",
     "timestamp": 1594363186070,
     "user": {
      "displayName": "Anton Savostianov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjBbdK6s_Mn01lJaBnVOEN8yDfMrLpHYHBGDcZ8Uw=s64",
      "userId": "02854421325018836691"
     },
     "user_tz": -120
    },
    "id": "xlhktOlMBED1",
    "outputId": "8694bc80-8710-449e-99b8-34bdabd4c55f"
   },
   "outputs": [],
   "source": [
    "gamma = E_step(X, pi0, mu0, sigma0)\n",
    "print(gamma[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fERrQWSCBED5"
   },
   "source": [
    "### M-step\n",
    "\n",
    "In M-step we need to maximize $\\mathbb{E}_{q(T)}\\log p(X,T | \\theta)$ with respect to $\\theta$. In our model this means that we need to find optimal values of $\\pi$, $\\mu$, $\\Sigma$. We start by deriving formulas for $\\mu$ as it is the easiest part. Then move on to $\\Sigma$. Finaly, to compute $\\pi$, we need <a href=\"https://www3.nd.edu/~jstiver/FIN360/Constrained%20Optimization.pdf\">Lagrange Multipliers technique</a> to satisfy constraint $\\sum\\limits_{i=1}^{n}\\pi_i = 1$.\n",
    "\n",
    "<br>\n",
    "<b>Note:</b> We need to compute derivatives of scalars with respect to matrices. The necessary information can be found on<a href=\"https://en.wikipedia.org/wiki/Matrix_calculus\"> wiki article</a> about it . Main formulas of matrix derivatives can be found in <a href=\"http://www2.imm.dtu.dk/pubdb/views/edoc_download.php/3274/pdf/imm3274.pdf\">Chapter 2 of The Matrix Cookbook</a>. One of the useful formulas is $\\frac{\\partial}{\\partial A}\\log |A| = A^{-T}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_ZbqWMKZa_5y"
   },
   "source": [
    "Maximization of $\\mathbb{E}_{q(T)}\\log p(X,T | \\theta) = \\sum\\limits_{i=1}^N\\sum\\limits_{c=1}^C q(t_i=c)\\log \\left(\\frac{p(x_i, t_i = c\\mid \\theta)}{q(t_i=c)}\\right)$ with respect to $\\theta$ for $q(t_i)$ fixed is equivalent to maximization of the expression\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{M} = \\sum_{i=1}^N\\sum_{c=1}^C q(t_i=c)\\log \\left(p(x_i, t_i = c\\mid \\theta)\\right) = \n",
    "\\sum_{i=1}^N\\sum_{c=1}^C \\gamma_{ic}\\log \\left(p(x_i\\mid t_i = c, \\theta)\\pi_c\\right) = \n",
    "\\sum_{i=1}^N\\sum_{c=1}^C \\gamma_{ic}\\log \\left(f_{\\!\\mathcal{N}}(x_i\\mid \\mu_c, \\Sigma_c)\\pi_c\\right) = \\\\ \n",
    "\\sum_{i=1}^N\\sum_{c=1}^C \\gamma_{ic}\\left(-\\frac{1}{2}\\Sigma_c^{-1}(x_i-\\mu_c).(x_i-\\mu_c) + \\log(\\pi_c) - \\frac{d}{2}\\log(2\\pi) - \\frac{1}{2}\\log|\\det\\Sigma_c| \\right).\n",
    "\\end{align}\n",
    "\n",
    "### Step 1: $\\mu_c$.\n",
    "\n",
    "Using that for $A^t=A$ we have $\\nabla\\frac{1}{2}(Ax.x) = Ax$, one finds\n",
    "\n",
    "\\begin{equation*}\n",
    "\\partial_{\\mu_c}\\mathcal{M} = \\sum_{i=1}^N\\gamma_{ic}\\left(\\Sigma_c^{-1}(x_i - \\mu_c)\\right) = \\Sigma_c^{-1}\\left(\\sum_{i=1}^N\\gamma_{ic}(x_i-\\mu_c)\\right)=0, \n",
    "\\end{equation*}\n",
    "that is \n",
    "\\begin{equation*}\n",
    "\\mu_c = \\frac{\\sum_{i=1}^N\\gamma_{ic}\\,x_i}{\\sum_{i=1}^N\\gamma_{ic}}.\n",
    "\\end{equation*}\n",
    "\n",
    "### Step 2: $\\Sigma_c$.\n",
    "\n",
    "We notice that:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathcal{M} = \\sum_{i=1}^N\\sum_{c=1}^C \\gamma_{ic}\\left(-\\frac{1}{2}\\Sigma_c^{-1}(x_i-\\mu_c).(x_i-\\mu_c) + \\log(\\pi_c) - \\frac{n}{2}\\log(2\\pi) + \\frac{1}{2}\\log|\\det\\Sigma_c^{-1}| \\right),\n",
    "\\end{equation*}\n",
    "so it is more convenient to find $\\partial_{\\Sigma_c^{-1}}\\mathcal{M}$.\n",
    "\n",
    "Using that \n",
    "\\begin{equation*}\n",
    "\\partial_A \\left(\\frac{1}{2}Ax.x\\right) = \\frac{1}{2}x\\otimes x, \\qquad \\partial_A\\ln |\\det A| = (A^{-1})^t,\n",
    "\\end{equation*}\n",
    "we find (taking into account that $\\Sigma_c^t=\\Sigma_c$)\n",
    "\\begin{equation*}\n",
    "\\partial_{\\Sigma_c^{-1}}\\mathcal{M} = \\sum_{i=1}^N\\gamma_{ic}\\left(-\\frac{1}{2}(x_i-\\mu_c)\\otimes (x_i-\\mu_c) + \\frac{1}{2}\\Sigma_c\\right)=0,\n",
    "\\end{equation*}\n",
    "that implies\n",
    "\\begin{equation*}\n",
    "\\Sigma_c = \\frac{\\sum_{i=1}^N\\gamma_{ic}(x_i-\\mu_c)\\otimes (x_i-\\mu_c)}{\\sum_{i=1}^N\\gamma_{ic}}.\n",
    "\\end{equation*}\n",
    "\n",
    "### Step 3: $\\pi_c$.\n",
    "\n",
    "We want to solve the problem\n",
    "\\begin{equation}\n",
    "\\label{pic1}\n",
    "\\begin{cases}\n",
    "\\tag{$\\pi_c:1$}\n",
    "\\mathcal{M} \\to \\max\\limits_{\\{\\pi_c\\}},\\\\\n",
    "\\sum\\limits_{c=1}^C\\pi_c=1,\\ and\\ \\pi_c\\geq 0,\\ c\\in C.\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "Setting\n",
    "\\begin{equation*}\n",
    "\\gamma_c = \\sum_{i=1}^N \\gamma_{ic},\\ c \\in \\overline{1,C}.\n",
    "\\end{equation*}\n",
    "\n",
    "Obviously, the above optimization problem is equivalent to the following one\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{pic2}\n",
    "\\begin{cases}\n",
    "\\tag{$\\pi_c:2$}\n",
    "\\sum\\limits_{c=1}^C\\gamma_c\\log\\pi_c \\to \\max\\limits_{\\{\\pi_c\\}},\\\\\n",
    "\\sum\\limits_{c=1}^C\\pi_c=1,\\ and\\ \\pi_c\\geq 0,\\ c\\in C,\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "or equivalently\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{pic3}\n",
    "\\begin{cases}\n",
    "\\tag{$\\pi_c:3$}\n",
    "-\\sum\\limits_{c=1}^C\\gamma_c\\log\\pi_c \\to \\min\\limits_{\\{\\pi_c\\}},\\\\\n",
    "\\sum\\limits_{c=1}^C\\pi_c-1=0,\\ and\\ -\\pi_c\\leq 0,\\ c\\in C.\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "Let us write down the Lagrange function:\n",
    "\\begin{equation*}\n",
    "La = -\\sum\\limits_{c=1}^C\\gamma_c\\log\\pi_c +\\alpha\\left(\\sum\\limits_{c=1}^C\\pi_c-1\\right)+\\sum\\limits_{c=1}^C\\beta_c(-\\pi_c).\n",
    "\\end{equation*}\n",
    "\n",
    "The neccesary conditions of the extremum reads\n",
    "\\begin{equation*}\n",
    "\\begin{cases}\n",
    "\\partial_{\\pi_c}La = -\\frac{\\gamma_c}{\\pi_c}+\\alpha-\\beta_c=0,\\ c \\in \\overline{1,C};\\\\\n",
    "\\beta_c\\pi_c=0,\\ \\beta_c\\geq 0,\\ c \\in \\overline{1,C};\\\\\n",
    "\\sum\\limits_{c=1}^C\\pi_c=1.\n",
    "\\end{cases}\n",
    "\\end{equation*}\n",
    "From this system we observe:\n",
    "\\begin{align}\n",
    "&-\\gamma_c+\\alpha\\pi_c-\\beta_c\\pi_c=0 \\quad \\Rightarrow \\quad -\\gamma_c+\\alpha\\pi_c = 0 \\quad \\Rightarrow\\\\\n",
    "& \\pi_c = \\frac{\\gamma_c}{\\alpha}\\quad \\Rightarrow \\quad 1=\\frac{\\sum^C_{c=1}\\gamma_c}{\\alpha}= \\frac{C}{\\alpha} \\quad \\Rightarrow \\quad \\alpha = C \\quad \\Rightarrow\\\\\n",
    "&\\pi_c = \\frac{\\gamma_c}{C},\\ c \\in \\overline{1,C}.\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 843,
     "status": "ok",
     "timestamp": 1594367933309,
     "user": {
      "displayName": "Anton Savostianov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjBbdK6s_Mn01lJaBnVOEN8yDfMrLpHYHBGDcZ8Uw=s64",
      "userId": "02854421325018836691"
     },
     "user_tz": -120
    },
    "id": "mhOr5I1bBED7"
   },
   "outputs": [],
   "source": [
    "def M_step(X, gamma):\n",
    "    \"\"\"\n",
    "    Performs M-step on GMM model\n",
    "    Each input is numpy array:\n",
    "    X: (N x d), data points\n",
    "    gamma: (N x C), distribution q(T)  \n",
    "    \n",
    "    Returns:\n",
    "    pi: (C)\n",
    "    mu: (C x d)\n",
    "    sigma: (C x d x d)\n",
    "    \"\"\"\n",
    "    N = X.shape[0] # number of objects\n",
    "    C = gamma.shape[1] # number of clusters\n",
    "    d = X.shape[1] # dimension of each object\n",
    "    \n",
    "    gamma_c = np.sum(gamma, axis = 0)\n",
    "    \n",
    "    mu = gamma.T@X\n",
    "    mu = ((mu.T)/gamma_c).T\n",
    "    \n",
    "    pi = gamma_c/C\n",
    "    \n",
    "    sigma = np.zeros((C,d,d))\n",
    "    \n",
    "    for c in range(C):\n",
    "        \n",
    "        # v.shape - N x d\n",
    "        v = X - mu[c]\n",
    "        \n",
    "        sigma[c] = np.array([np.sum(gamma[:,c]*v[:,m]*v[:,n]) for m in range(d) for n in range(d)]).reshape((d,d))\n",
    "        sigma[c] = sigma[c]/gamma_c[c]\n",
    "        \n",
    "    return pi, mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 830,
     "status": "ok",
     "timestamp": 1594368047113,
     "user": {
      "displayName": "Anton Savostianov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjBbdK6s_Mn01lJaBnVOEN8yDfMrLpHYHBGDcZ8Uw=s64",
      "userId": "02854421325018836691"
     },
     "user_tz": -120
    },
    "id": "6i5cv65SBED-",
    "outputId": "24bd954d-0a6c-4bcb-beef-69fa0c067bc8"
   },
   "outputs": [],
   "source": [
    "# check\n",
    "sigma1 = np.array([np.identity(2) for c in range(3)])\n",
    "gamma = E_step(X, pi0, mu0, sigma1)\n",
    "pi, mu, sigma = M_step(X, gamma)\n",
    "print(\"det(sigma)\",det(sigma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "svyzjt7XBEEC"
   },
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qexOXBLUBEED"
   },
   "source": [
    "Finally, we need some function to track convergence. We will use variational lower bound $\\mathcal L = \\mathcal{L}(\\theta,q)$ for this purpose. We will stop our EM iterations when $\\mathcal{L}$ will saturate. Usually, we need only about 10-20 iterations to converge. It is also useful to check that this function never decreases during training. If it does, there is a bug in the code.\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{L} = \\sum_{i=1}^{N} \\sum_{c=1}^{C} q(t_i =c) (\\log \\pi_c + \\log f_{\\!\\mathcal{N}}(x_i \\mid \\mu_c, \\Sigma_c)) - \\sum_{i=1}^{N} \\sum_{c=1}^{K} q(t_i =c) \\log q(t_i =c)=\\\\\n",
    "\\sum_{i=1}^N\\sum_{c=1}^C \\gamma_{ic}\\left(-\\frac{1}{2}\\Sigma_c^{-1}(x_i-\\mu_c).(x_i-\\mu_c) + \\log(\\pi_c) - \\frac{n}{2}\\log(2\\pi) - \\frac{1}{2}\\log|\\det\\Sigma_c| \\right) - \\sum_{i=1}^N\\sum_{c=1}^C\\gamma_{ic}\\log(\\gamma_{ic}).\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1074,
     "status": "ok",
     "timestamp": 1594368059412,
     "user": {
      "displayName": "Anton Savostianov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjBbdK6s_Mn01lJaBnVOEN8yDfMrLpHYHBGDcZ8Uw=s64",
      "userId": "02854421325018836691"
     },
     "user_tz": -120
    },
    "id": "B5tKCZe0BEEE"
   },
   "outputs": [],
   "source": [
    "def compute_vlb(X, pi, mu, sigma, gamma):\n",
    "    \"\"\"\n",
    "    Each input is numpy array:\n",
    "    X: (N x d), data points\n",
    "    gamma: (N x C), distribution q(T)  \n",
    "    pi: (C)\n",
    "    mu: (C x d)\n",
    "    sigma: (C x d x d)\n",
    "    \n",
    "    Returns value of variational lower bound\n",
    "    \"\"\"\n",
    "    N = X.shape[0] # number of objects\n",
    "    C = gamma.shape[1] # number of clusters\n",
    "    d = X.shape[1] # dimension of each object\n",
    "    \n",
    "    log_sigma = np.linalg.slogdet(sigma)[1]\n",
    "    \n",
    "    logits = np.zeros((C,N))\n",
    "    \n",
    "    for c in range(C):\n",
    "        yc = np.linalg.solve(sigma[c],(X - mu[c]).T).T\n",
    "        logits[c] = -0.5 * np.sum(yc*(X-mu[c]),axis = 1) + np.log(pi[c])-np.log(2*np.pi) - 0.5*log_sigma[c]\n",
    "    logits = logits.T\n",
    "    \n",
    "    loss = np.sum(gamma*logits - gamma*np.log(gamma))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 830,
     "status": "ok",
     "timestamp": 1594368073529,
     "user": {
      "displayName": "Anton Savostianov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjBbdK6s_Mn01lJaBnVOEN8yDfMrLpHYHBGDcZ8Uw=s64",
      "userId": "02854421325018836691"
     },
     "user_tz": -120
    },
    "id": "xNva3XRTBEEI",
    "outputId": "f85d2d5a-7eae-4603-a5ad-d41fb8202a0a"
   },
   "outputs": [],
   "source": [
    "pi, mu, sigma = pi0, mu0, sigma0\n",
    "gamma = E_step(X, pi, mu, sigma)\n",
    "pi, mu, sigma = M_step(X, gamma)\n",
    "loss = compute_vlb(X, pi, mu, sigma, gamma)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I5p8XC-eBEEM"
   },
   "source": [
    "### Bringing it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cQsdsNVCBEEU"
   },
   "source": [
    "Now that we have E step, M step and VLB, we can implement the training loop. We will initialize values of $\\pi$, $\\mu$ and $\\Sigma$ to some random numbers, train until $\\mathcal{L}$ stops changing, and return the resulting points. We also know that the EM algorithm converges to local optima. To find a better local optima, we will restart the algorithm multiple times from different (random) starting positions. Each training trial should stop either when maximum number of iterations is reached or when relative improvement is smaller than given tolerance ($|\\frac{\\mathcal{L}_i-\\mathcal{L}_{i-1}}{\\mathcal{L}_{i-1}}| \\le \\text{rtol}$).\n",
    "\n",
    "Remember, that initial (random) values of $\\pi$ that you generate must be non-negative and sum up to 1. We use $\\Sigma=I$ as initialization.\n",
    "\n",
    "Sometimes we get numerical errors because of component collapsing. The easiest way to deal with this problems is to restart the procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 885,
     "status": "ok",
     "timestamp": 1594369441698,
     "user": {
      "displayName": "Anton Savostianov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjBbdK6s_Mn01lJaBnVOEN8yDfMrLpHYHBGDcZ8Uw=s64",
      "userId": "02854421325018836691"
     },
     "user_tz": -120
    },
    "id": "Q1AAcyl0BEEW"
   },
   "outputs": [],
   "source": [
    "def train_EM(X, C, rtol=1e-3, max_iter=100, restarts=10):\n",
    "    '''\n",
    "    Starts with random initialization *restarts* times\n",
    "    Runs optimization until saturation with *rtol* reached\n",
    "    or *max_iter* iterations were made.\n",
    "    \n",
    "    X: (N, d), data points\n",
    "    C: int, number of clusters\n",
    "    '''\n",
    "    N = X.shape[0] # number of objects\n",
    "    d = X.shape[1] # dimension of each object\n",
    "    best_loss = None\n",
    "    best_pi = None\n",
    "    best_mu = None\n",
    "    best_sigma = None    \n",
    "    \n",
    "    \n",
    "    rng = default_rng()\n",
    "\n",
    "    for trial in range(restarts):\n",
    "        try:\n",
    "            i, rtol_cur = 0, 1\n",
    "            \n",
    "            pi_old = softmax(rng.random(C))\n",
    "            mu_old = 8*rng.random((C,d)) # numbers in the square (0,8)x(0,8)\n",
    "\n",
    "            sigma_old = np.array([np.identity(d) for c in range(C)]) \n",
    "            \n",
    "            loss_history = []\n",
    "            \n",
    "            while (i<max_iter) and (rtol_cur > rtol):\n",
    "                i=i+1\n",
    "                gamma = E_step(X, pi_old, mu_old, sigma_old)\n",
    "                pi_new, mu_new, sigma_new = M_step(X,gamma)\n",
    "                loss = compute_vlb(X, pi_new, mu_new, sigma_new, gamma)\n",
    "                loss_history.append(loss)\n",
    "                pi_old, mu_old, sigma_old = pi_new, mu_new, sigma_new\n",
    "\n",
    "                if i==1:\n",
    "                    rtol_cur = 1\n",
    "                    loss_old = loss\n",
    "                if i>1:\n",
    "                    rtol_cur = (loss-loss_old)/(loss_old)\n",
    "                    loss_old = loss\n",
    "                \n",
    "                if (best_loss == None) or (loss > best_loss):\n",
    "                    # condition loss>best_loss since loss = L(thetea,q) is maximized\n",
    "                    best_loss = loss\n",
    "                    best_pi, best_mu, best_sigma = pi_new, mu_new, sigma_new\n",
    "            \n",
    "            #clear_output()\n",
    "            print(\"trial\",trial,\": loss\",loss)\n",
    "            plt.plot(np.array(loss_history))\n",
    "            plt.show()\n",
    "            plt.clf()\n",
    "          \n",
    "        except np.linalg.LinAlgError:\n",
    "            print(\"trial\",trial, \";\", \"iteration\",i)\n",
    "            print(\"Singular matrix: components collapsed\")\n",
    "            print(\"det(sigma_old)\", det(sigma_old))\n",
    "            print()\n",
    "            pass\n",
    "\n",
    "    return best_loss, best_pi, best_mu, best_sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2938,
     "status": "ok",
     "timestamp": 1594369449428,
     "user": {
      "displayName": "Anton Savostianov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjBbdK6s_Mn01lJaBnVOEN8yDfMrLpHYHBGDcZ8Uw=s64",
      "userId": "02854421325018836691"
     },
     "user_tz": -120
    },
    "id": "0f8A5sbmBEEZ",
    "outputId": "59428f91-f2da-4b66-d4b4-ccebbf4aec84"
   },
   "outputs": [],
   "source": [
    "best_loss, best_pi, best_mu, best_sigma = train_EM(X, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tYd6tPHKBEEd"
   },
   "source": [
    "Let's plot the clusters to see it. We will assign a cluster label as the most probable cluster index. This can be found using a matrix $\\gamma$ computed on last E-step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1288,
     "status": "ok",
     "timestamp": 1594369473989,
     "user": {
      "displayName": "Anton Savostianov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjBbdK6s_Mn01lJaBnVOEN8yDfMrLpHYHBGDcZ8Uw=s64",
      "userId": "02854421325018836691"
     },
     "user_tz": -120
    },
    "id": "mK_M6QLnBEEe",
    "outputId": "7ba4d5e1-7f6a-49e4-bcea-7451f7527b2e"
   },
   "outputs": [],
   "source": [
    "gamma = E_step(X, best_pi, best_mu, best_sigma)\n",
    "labels = gamma.argmax(axis=1)\n",
    "colors = np.array([(31, 119, 180), (255, 127, 14), (44, 160, 44)]) / 255.\n",
    "plt.scatter(X[:, 0], X[:, 1], c=colors[labels], s=30)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "em-my.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
